{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "5e69ca2c1e5146648771625c0e63789b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_292565856e85437c8e34c743dbec12ef",
              "IPY_MODEL_ede861db50ad4c6dbb02a76b4bb71c1a",
              "IPY_MODEL_1e4b78b0536c4bfc8e9566c1b81e7e92"
            ],
            "layout": "IPY_MODEL_957cd6d81275419fa38610ff2e6acbc6"
          }
        },
        "292565856e85437c8e34c743dbec12ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_9435bf9250c240eba7d4887850f9f4bd",
            "placeholder": "​",
            "style": "IPY_MODEL_8de983d7a1c1400f974f2a9aecfa4951",
            "value": "model.safetensors: 100%"
          }
        },
        "ede861db50ad4c6dbb02a76b4bb71c1a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7102da0a90be47a795d5e3593a891955",
            "max": 346284714,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_27a679338d104854b540569c21acb2e4",
            "value": 346284714
          }
        },
        "1e4b78b0536c4bfc8e9566c1b81e7e92": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a96b11c7e6ad4829836806be2478c1d2",
            "placeholder": "​",
            "style": "IPY_MODEL_2a03aa136e574e479e2a6f158273afb0",
            "value": " 346M/346M [00:01&lt;00:00, 255MB/s]"
          }
        },
        "957cd6d81275419fa38610ff2e6acbc6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "9435bf9250c240eba7d4887850f9f4bd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "8de983d7a1c1400f974f2a9aecfa4951": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "7102da0a90be47a795d5e3593a891955": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "27a679338d104854b540569c21acb2e4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "a96b11c7e6ad4829836806be2478c1d2": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2a03aa136e574e479e2a6f158273afb0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# DL-CBIR using ViT Base-16 Architecture\n",
        "\n",
        "**Offline Phase**\n",
        "***"
      ],
      "metadata": {
        "id": "9mO26uw2UNQg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Libraries"
      ],
      "metadata": {
        "id": "gODwD3yIU8LO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "\n",
        "import os\n",
        "import logging\n",
        "from PIL import Image\n",
        "\n",
        "import ast\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import joblib\n",
        "from joblib import load\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Extras\n",
        "import time\n",
        "from google.colab import userdata"
      ],
      "metadata": {
        "id": "tmyinl-SU5cr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# HuggingFace Token\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ],
      "metadata": {
        "id": "Kl30ey_Z8FCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ],
      "metadata": {
        "id": "7c5NuDLg7aWj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99d352f9-476f-443b-f68d-e201873cfc36"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x78b3c1130cf0>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Detect Device"
      ],
      "metadata": {
        "id": "PjYznWJVsuDe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "o4TAMusAsqWU",
        "outputId": "113a7ad8-5989-4e89-8a23-1153f6fca94a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Configuration Parameters"
      ],
      "metadata": {
        "id": "054K6K0oVB9_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "DIR_PATH = '/content/drive/MyDrive/ML_Datasets/corel_1k_dataset/'\n",
        "FEATURES_PATH = '/content/drive/MyDrive/ML_Datasets/vit_b_16_feature_vectors.csv'\n",
        "MODELS_PATH = '/content/drive/MyDrive/ML_Models/'\n",
        "svm_model_path = MODELS_PATH + 'vit_b_16_svm_model.joblib'\n",
        "\n",
        "TRAIN_SPLIT = 0.8\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "N_WORKERS = 2\n",
        "\n",
        "PCA_DIM = 64\n",
        "N_RESULTS = 10"
      ],
      "metadata": {
        "id": "QBSGwcftU0DH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Logging Setup"
      ],
      "metadata": {
        "id": "5WPq4yBHVAOy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Remove any existing handlers\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Add a new stream handler\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)"
      ],
      "metadata": {
        "id": "HdIlnNIWU2wY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Connecting to the dataset"
      ],
      "metadata": {
        "id": "FcsMXUldo4We"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "drive.mount('/content/drive')\n",
        "print(f\"Available classes: {os.listdir(DIR_PATH + '/training_set')}\")\n",
        "print(f\"Number of classes: {len(os.listdir(DIR_PATH + '/training_set'))}\")"
      ],
      "metadata": {
        "id": "rhK9_jM9o3bu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c02bcbf0-f9de-4930-9c59-56144be43a43"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Available classes: ['flowers', 'bus', 'foods', 'monuments', 'dinosaurs', 'peolpe_and_villages_in_Africa', 'elephants', 'horses', 'beaches', 'mountains_and_snow']\n",
            "Number of classes: 10\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Components"
      ],
      "metadata": {
        "id": "oUv4vOdA4Z2w"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data Loader"
      ],
      "metadata": {
        "id": "WRMstN62VFSY"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QoktGn_6Uubh"
      },
      "outputs": [],
      "source": [
        "def get_data_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def load_images_and_labels(data_dir, target_class=None):\n",
        "    \"\"\"\n",
        "    Loads images (optionally only from a specific class) and returns:\n",
        "      - features (torch.Tensor) of shape (N, C, H, W) on DEVICE\n",
        "      - labels   (torch.Tensor) of shape (N,) on DEVICE\n",
        "      - file_names (list) of file paths for each image\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory path containing class subdirectories.\n",
        "        target_class (str or list[str], optional): Only load images from this class (or list of classes).\n",
        "                                                   If None, loads all classes.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        raise FileNotFoundError(f\"Data directory {data_dir} not found.\")\n",
        "\n",
        "    transform_pipeline = get_data_transforms()\n",
        "    dataset = datasets.ImageFolder(root=data_dir)  # no transform yet\n",
        "\n",
        "    # Determine which class indices to keep\n",
        "    if target_class is None:\n",
        "        valid_class_idxs = set(dataset.class_to_idx.values())\n",
        "    else:\n",
        "        if isinstance(target_class, str):\n",
        "            target_list = [target_class]\n",
        "        else:\n",
        "            target_list = list(target_class)\n",
        "        valid_class_idxs = {dataset.class_to_idx[c] for c in target_list}\n",
        "\n",
        "    # Filter samples\n",
        "    filtered_samples = [\n",
        "        (path, label)\n",
        "        for (path, label) in dataset.samples\n",
        "        if label in valid_class_idxs\n",
        "    ]\n",
        "\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    file_names  = []\n",
        "\n",
        "    for img_path, label in filtered_samples:\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_t = transform_pipeline(img)\n",
        "        feature_list.append(img_t)\n",
        "        label_list.append(label)\n",
        "        file_names.append(img_path)\n",
        "\n",
        "    # Stack and move to DEVICE\n",
        "    features = torch.stack(feature_list).to(DEVICE)      # (N, C, H, W)\n",
        "    labels   = torch.tensor(label_list, device=DEVICE)   # (N,)\n",
        "\n",
        "    return features, labels, file_names"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Pre-trained ViT Base-16 Model for Feature Extraction"
      ],
      "metadata": {
        "id": "V0nBA_U0rmU5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ViTFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTFeatureExtractor, self).__init__()\n",
        "        # Load pre‑trained ViT and remove its classification head\n",
        "        self.model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.model.head = nn.Identity()\n",
        "        # Move to DEVICE and set to eval mode\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extracts ViT features for a batch of images.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images as a tensor of shape (N, C, H, W),\n",
        "                              already resized/cropped/normalized to 224×224.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Feature tensor of shape (N, embed_dim) on DEVICE.\n",
        "        \"\"\"\n",
        "        # Ensure input is on DEVICE\n",
        "        x = x.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            features = self.model(x)\n",
        "        return features"
      ],
      "metadata": {
        "id": "iYD8aHJzri8S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## PCA for Feature Extraction"
      ],
      "metadata": {
        "id": "_qqgynMdUoQt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_normalized_pca(features: torch.Tensor, reduced_dim: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs PCA-based dimensionality reduction on a tensor of AlexNet features (N, 4096)\n",
        "    followed by L2 normalization. The output always has shape (N, reduced_di'flowers'm), even if the\n",
        "    number of available principal components (i.e. batch size N) is less than reduced_dim.\n",
        "    In such cases, the projection matrix is padded with zeros. All computation happens on DEVICE.\n",
        "\n",
        "    Args:\n",
        "        features (torch.Tensor): Input tensor of shape (N, 4096), where N is the number of images.\n",
        "        reduced_dim (int): Target dimensionality after PCA.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: L2‑normalized reduced features of shape (N, reduced_dim), on DEVICE.\n",
        "    \"\"\"\n",
        "    # Move features to DEVICE\n",
        "    features = features.to(DEVICE)\n",
        "\n",
        "    # Step 1: Center the data\n",
        "    mean = features.mean(dim=0, keepdim=True)               # (1, 4096)\n",
        "    features_centered = features - mean                     # (N, 4096)\n",
        "\n",
        "    # Step 2: Compute SVD on the centered data\n",
        "    U, S, Vh = torch.linalg.svd(features_centered, full_matrices=False)\n",
        "    # Vh has shape (min(N, 4096), 4096)\n",
        "    num_components = Vh.shape[0]\n",
        "\n",
        "    # Step 3: Select top principal components and form projection matrix\n",
        "    if num_components >= reduced_dim:\n",
        "        # Enough components\n",
        "        principal_components = Vh[:reduced_dim].T          # (4096, reduced_dim)\n",
        "    else:\n",
        "        # Pad with zeros\n",
        "        principal_components = Vh[:num_components].T       # (4096, num_components)\n",
        "        pad_width = reduced_dim - num_components\n",
        "        padding = torch.zeros((features.shape[1], pad_width), device=DEVICE)\n",
        "        principal_components = torch.cat([principal_components, padding], dim=1)  # (4096, reduced_dim)\n",
        "\n",
        "    # Step 4: Project onto the reduced subspace\n",
        "    reduced_features = features_centered @ principal_components  # (N, reduced_dim)\n",
        "\n",
        "    # Step 5: L2‑normalize each row\n",
        "    reduced_features_normalized = F.normalize(reduced_features, p=2, dim=1)  # (N, reduced_dim)\n",
        "\n",
        "    return reduced_features_normalized\n"
      ],
      "metadata": {
        "id": "hMzK-o7TUrBv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## DCT for Feature Extraction"
      ],
      "metadata": {
        "id": "Zh98juc2VH1l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_normalized_dct(images: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the L2‑normalized DCT feature vector for each image in a batch,\n",
        "    returning a tensor on DEVICE.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images with shape (N, C, H, W),\n",
        "                               where N is the number of images and C is the number of channels (3).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of L2‑normalized DCT feature vectors with shape (N, feature_length),\n",
        "                      on DEVICE.\n",
        "    \"\"\"\n",
        "    # Move to CPU and convert to HWC numpy array\n",
        "    images_np = images.permute(0, 2, 3, 1).cpu().numpy()  # shape: (N, H, W, C)\n",
        "\n",
        "    dct_features = []\n",
        "    for img in images_np:\n",
        "        # Grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        # float32\n",
        "        gray_f = np.float32(gray)\n",
        "        # DCT\n",
        "        dct_res = cv2.dct(gray_f)\n",
        "        # Flatten\n",
        "        vec = dct_res.flatten()\n",
        "        # L2 norm\n",
        "        norm = np.linalg.norm(vec)\n",
        "        if norm > 0:\n",
        "            vec = vec / norm\n",
        "        dct_features.append(vec)\n",
        "\n",
        "    # Stack into numpy, convert to torch tensor, move to DEVICE\n",
        "    dct_np = np.stack(dct_features, axis=0)           # shape: (N, feature_length)\n",
        "    dct_tensor = torch.from_numpy(dct_np).to(DEVICE)  # on DEVICE\n",
        "\n",
        "    return dct_tensor"
      ],
      "metadata": {
        "id": "_O_HlvycVY6D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Feature Vector Combination"
      ],
      "metadata": {
        "id": "RsmwicLVsv0q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def compute_combined_feature_vector(\n",
        "    images_tensor: torch.Tensor,\n",
        "    alexnet_features: torch.Tensor,\n",
        "    pca_dim: int = PCA_DIM\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Combined Feature Vectors by concatenating the normalized DCT and PCA feature vectors\n",
        "    for a batch of images, performing all computations on DEVICE.\n",
        "\n",
        "    Args:\n",
        "        images_tensor (torch.Tensor): Input image tensor with shape (N, C, H, W).\n",
        "        alexnet_features (torch.Tensor): AlexNet feature tensor with shape (N, 4096).\n",
        "        pca_dim (int): Target dimensionality for PCA reduction.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Combined feature vectors of shape (N, dct_length + pca_dim), on DEVICE.\n",
        "    \"\"\"\n",
        "    # Move inputs to DEVICE\n",
        "    images_tensor = images_tensor.to(DEVICE)\n",
        "    alexnet_features = alexnet_features.to(DEVICE)\n",
        "\n",
        "    # Compute normalized DCT feature vectors (on DEVICE)\n",
        "    dct_features = compute_normalized_dct(images_tensor)    # shape: (N, dct_length)\n",
        "\n",
        "    # Compute normalized PCA feature vectors (on DEVICE)\n",
        "    pca_features = compute_normalized_pca(alexnet_features, pca_dim)  # shape: (N, pca_dim)\n",
        "\n",
        "    # Concatenate along feature axis (result is on DEVICE)\n",
        "    combined_feature_vectors = torch.cat((dct_features, pca_features), dim=1)\n",
        "\n",
        "    return combined_feature_vectors"
      ],
      "metadata": {
        "id": "4TVigE3Zs01h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Features Database"
      ],
      "metadata": {
        "id": "jCcbzcfUICj4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_feature_vectors_csv(feature_vectors: torch.Tensor, labels: torch.Tensor, file_names: list, output_csv_path: str):\n",
        "    \"\"\"\n",
        "    Generates a CSV file with columns ordered as: file_path, label, f1, f2, ..., fN.\n",
        "    This structure is preferred for CBIR applications, where each row represents an image,\n",
        "    and each feature value is stored in its own column.\n",
        "\n",
        "    Args:\n",
        "        feature_vectors (torch.Tensor): Tensor of combined feature vectors with shape (N, feature_length).\n",
        "        labels (torch.Tensor): Tensor of labels with shape (N,).\n",
        "        file_names (list): List of file paths corresponding to each image.\n",
        "        output_csv_path (str): Full path (including filename) where the CSV file will be saved.\n",
        "    \"\"\"\n",
        "    if feature_vectors.size(0) == 0:\n",
        "        raise ValueError(\"Feature vectors tensor is empty!\")\n",
        "\n",
        "    # Convert tensors to numpy arrays.\n",
        "    feature_vectors_np = feature_vectors.cpu().numpy()\n",
        "    labels_np = labels.cpu().numpy()\n",
        "\n",
        "    # Determine the feature vector length.\n",
        "    feature_length = feature_vectors_np.shape[1]\n",
        "\n",
        "    # Create the column names: file_path, label, f1, f2, ..., fN.\n",
        "    columns = ['file_path', 'label'] + [f'f{i+1}' for i in range(feature_length)]\n",
        "\n",
        "    data = []\n",
        "    for feat, label, fname in zip(feature_vectors_np, labels_np, file_names):\n",
        "        # Each row contains the file path, label, then each feature value.\n",
        "        row = [fname, label] + feat.tolist()\n",
        "        data.append(row)\n",
        "\n",
        "    # Create the DataFrame.\n",
        "    df = pd.DataFrame(data, columns=columns)\n",
        "\n",
        "    # Ensure the directory exists.\n",
        "    output_dir = os.path.dirname(output_csv_path)\n",
        "    if output_dir and not os.path.exists(output_dir):\n",
        "        os.makedirs(output_dir)\n",
        "\n",
        "    # Write the DataFrame to a CSV file.\n",
        "    df.to_csv(output_csv_path, index=False)\n",
        "    print(f\"CSV file '{output_csv_path}' generated successfully.\")"
      ],
      "metadata": {
        "id": "Z4RlLpfYITOT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVM Model for prediction of Image Class"
      ],
      "metadata": {
        "id": "AVeqUtvRWIB6"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Feature Vectors Dataset"
      ],
      "metadata": {
        "id": "FLRrSF3rw2I-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def load_feature_database(csv_file: str):\n",
        "    \"\"\"\n",
        "    Loads the image database from a CSV file and converts the 'feature_vector' column\n",
        "    from string to a list of floats.\n",
        "\n",
        "    Args:\n",
        "        csv_file (str): Path to the CSV file.\n",
        "\n",
        "    Returns:\n",
        "        tuple: A tuple (features, labels, file_paths) where:\n",
        "            - features is a numpy array of shape (N, d)\n",
        "            - labels is a list of labels\n",
        "            - file_paths is a list of file paths for each image\n",
        "    \"\"\"\n",
        "    data = pd.read_csv(csv_file)\n",
        "    # Convert the 'feature_vector' column from string to list of floats.\n",
        "    data['feature_vector'] = data['feature_vector'].apply(ast.literal_eval)\n",
        "    # Convert list of feature vectors into a numpy array.\n",
        "    features = np.array(data['feature_vector'].tolist())\n",
        "    labels = data['label'].tolist()\n",
        "    file_paths = data['file_path'].tolist()\n",
        "    return features, labels, file_paths"
      ],
      "metadata": {
        "id": "JwH7SAA6wy6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Model"
      ],
      "metadata": {
        "id": "M0mltQU_7OlS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class SVMClassifier:\n",
        "    def __init__(self, csv_file: str, label_column: str = 'label', test_size: float = 0.2, random_state: int = 42):\n",
        "        \"\"\"\n",
        "        Initializes the SVM classifier with data from a CSV file.\n",
        "\n",
        "        Args:\n",
        "            csv_file (str): Path to the input CSV file.\n",
        "            label_column (str): Name of the column containing class labels.\n",
        "            test_size (float): Fraction of data to be used for testing (default: 0.2).\n",
        "            random_state (int): Random seed for reproducibility (default: 42).\n",
        "        \"\"\"\n",
        "        self.csv_file = csv_file\n",
        "        self.label_column = label_column\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.X_test = None\n",
        "        self.y_test = None\n",
        "        logging.info(f\"SVMClassifier initialized with csv_file: {csv_file}, label_column: {label_column}\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads data from the CSV file and splits it into training and testing sets.\n",
        "        Expects the CSV file to have columns: file_path, label, f1, f2, ..., fN.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (X_train, X_test, y_train, y_test)\n",
        "        \"\"\"\n",
        "        logging.info(\"Loading data...\")\n",
        "        # Load the dataset\n",
        "        data = pd.read_csv(self.csv_file)\n",
        "\n",
        "        # Drop the file_path column if it exists (we don't use it for training)\n",
        "        if 'file_path' in data.columns:\n",
        "            data.drop(columns=['file_path'], inplace=True)\n",
        "\n",
        "        # Features are assumed to be all columns except the label column.\n",
        "        features = data.drop(columns=[self.label_column])\n",
        "        labels = data[self.label_column]\n",
        "\n",
        "        # Split into training and testing sets.\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features, labels, test_size=self.test_size, random_state=self.random_state\n",
        "        )\n",
        "        logging.info(\"Data loaded and split into training and test sets.\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the SVM model on the training set.\n",
        "        Standardizes the features and stores the trained model and scaler as instance attributes.\n",
        "\n",
        "        Returns:\n",
        "            model: The trained SVM model.\n",
        "        \"\"\"\n",
        "        logging.info(\"Starting training...\")\n",
        "        # Load and split the data.\n",
        "        X_train, X_test, y_train, y_test = self.load_data()\n",
        "\n",
        "        # Standardize features for better SVM performance.\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        self.scaler = scaler\n",
        "        self.X_test = X_test_scaled\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Initialize the SVM classifier with RBF kernel.\n",
        "        model = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
        "        logging.info(\"SVM model initialized with RBF kernel.\")\n",
        "\n",
        "        # Train the model.\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        self.model = model\n",
        "        logging.info(\"SVM model training completed.\")\n",
        "        return model\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the trained SVM model on the test set.\n",
        "        Prints the classification report and accuracy score.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (report, accuracy)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            logging.error(\"Model not trained. Please train the model before evaluation.\")\n",
        "            return None, None\n",
        "\n",
        "        logging.info(\"Starting evaluation...\")\n",
        "        # Make predictions on the test set.\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "\n",
        "        # Evaluate the model.\n",
        "        report = classification_report(self.y_test, y_pred)\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        logging.info(\"Evaluation completed.\")\n",
        "\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "        print(\"Accuracy Score:\", accuracy)\n",
        "        return report, accuracy\n",
        "\n",
        "    def save_model(self, filename: str):\n",
        "        \"\"\"\n",
        "        Saves the trained model and scaler to a file using joblib.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The file path where the model will be saved.\n",
        "        \"\"\"\n",
        "        if self.model is None or self.scaler is None:\n",
        "            logging.error(\"No trained model to save. Please train the model first.\")\n",
        "            return\n",
        "\n",
        "        joblib.dump((self.model, self.scaler), filename)\n",
        "        logging.info(f\"Model saved to {filename}\")\n",
        "\n",
        "    def load_model(self, filename: str):\n",
        "        \"\"\"\n",
        "        Loads a model and scaler from a file using joblib.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The file path from where the model will be loaded.\n",
        "        \"\"\"\n",
        "        self.model, self.scaler = joblib.load(filename)\n",
        "        logging.info(f\"Model loaded from {filename}\")\n"
      ],
      "metadata": {
        "id": "1-SvHt0rWKSs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Result"
      ],
      "metadata": {
        "id": "tRjn5grhuK7Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Load ALL classes:\n",
        "images_tensor, labels_tensor, file_names = load_images_and_labels(\n",
        "    DIR_PATH + '/training_set'\n",
        ")\n",
        "\n",
        "# # Load only the BUS class:\n",
        "# images_tensor, labels_tensor, file_names = load_images_and_labels(\n",
        "#     DIR_PATH + '/test_set', target_class='bus'\n",
        "# )\n",
        "\n",
        "# Load classes BUS and FLOWERS:\n",
        "# images_tensor, labels_tensor, file_names = load_images_and_labels(\n",
        "#     DIR_PATH + '/test_set', target_class=['bus','flowers']\n",
        "# )\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ],
      "metadata": {
        "id": "YosWgyxTtlaX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cfcdb6ea-3e5c-41e9-ad91-08db48e0c54e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 05:43:40,455 - INFO - Time taken to execute = 5.5826s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# ViT Model for Feature Extraction\n",
        "model = ViTFeatureExtractor()\n",
        "# The model expects a tensor of shape (N, C, H, W) and returns features of shape (N, 4096).\n",
        "model_features = model(images_tensor)\n",
        "logging.info(f\"Extracted features shape: {model_features.shape}\")\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ],
      "metadata": {
        "id": "Igxyosw7t6Lg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 138,
          "referenced_widgets": [
            "5e69ca2c1e5146648771625c0e63789b",
            "292565856e85437c8e34c743dbec12ef",
            "ede861db50ad4c6dbb02a76b4bb71c1a",
            "1e4b78b0536c4bfc8e9566c1b81e7e92",
            "957cd6d81275419fa38610ff2e6acbc6",
            "9435bf9250c240eba7d4887850f9f4bd",
            "8de983d7a1c1400f974f2a9aecfa4951",
            "7102da0a90be47a795d5e3593a891955",
            "27a679338d104854b540569c21acb2e4",
            "a96b11c7e6ad4829836806be2478c1d2",
            "2a03aa136e574e479e2a6f158273afb0"
          ]
        },
        "outputId": "dfaf5bb7-25b1-4b49-a17c-edbddabf7bfe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 05:43:42,583 - INFO - Loading pretrained weights from Hugging Face hub (timm/vit_base_patch16_224.augreg2_in21k_ft_in1k)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "model.safetensors:   0%|          | 0.00/346M [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "5e69ca2c1e5146648771625c0e63789b"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 05:43:45,103 - INFO - [timm/vit_base_patch16_224.augreg2_in21k_ft_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
            "2025-04-17 05:43:46,638 - INFO - Extracted features shape: torch.Size([900, 768])\n",
            "2025-04-17 05:43:46,639 - INFO - Time taken to execute = 4.0050s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Combined Feature Vector Generation\n",
        "combined_vectors = compute_combined_feature_vector(images_tensor, model_features)\n",
        "generate_feature_vectors_csv(combined_vectors, labels_tensor, file_names, FEATURES_PATH)\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ],
      "metadata": {
        "id": "Ir2J0KwluF3t",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fdf4c80c-e136-42e8-b5a4-614b9bd4471a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CSV file '/content/drive/MyDrive/ML_Datasets/vit_b_16_feature_vectors.csv' generated successfully.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 05:46:46,313 - INFO - Time taken to execute = 171.7949s\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Training the SVM Classifier\n",
        "label_column = 'label'\n",
        "\n",
        "svm_classifier = SVMClassifier(csv_file=FEATURES_PATH, label_column=label_column, test_size=0.2, random_state=42)\n",
        "model = svm_classifier.train()\n",
        "\n",
        "report, accuracy = svm_classifier.evaluate()\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ],
      "metadata": {
        "id": "ja96yWwNuhp1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "240221fa-ebad-44cd-e5f5-fa8d24453d62"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 05:46:46,319 - INFO - SVMClassifier initialized with csv_file: /content/drive/MyDrive/ML_Datasets/vit_b_16_feature_vectors.csv, label_column: label\n",
            "2025-04-17 05:46:46,322 - INFO - Starting training...\n",
            "2025-04-17 05:46:46,323 - INFO - Loading data...\n",
            "2025-04-17 05:47:26,131 - INFO - Data loaded and split into training and test sets.\n",
            "2025-04-17 05:47:27,719 - INFO - SVM model initialized with RBF kernel.\n",
            "2025-04-17 05:47:59,482 - INFO - SVM model training completed.\n",
            "2025-04-17 05:47:59,488 - INFO - Starting evaluation...\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "2025-04-17 05:48:10,438 - INFO - Evaluation completed.\n",
            "2025-04-17 05:48:10,447 - INFO - Time taken to execute = 109.1706s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00        23\n",
            "           1       0.11      0.58      0.19        12\n",
            "           2       0.33      0.79      0.47        19\n",
            "           3       0.00      0.00      0.00        19\n",
            "           4       0.68      0.81      0.74        21\n",
            "           5       0.00      0.00      0.00        20\n",
            "           6       0.11      0.06      0.08        17\n",
            "           7       0.00      0.00      0.00        14\n",
            "           8       0.00      0.00      0.00        20\n",
            "           9       0.16      0.40      0.23        15\n",
            "\n",
            "    accuracy                           0.26       180\n",
            "   macro avg       0.14      0.26      0.17       180\n",
            "weighted avg       0.15      0.26      0.17       180\n",
            "\n",
            "Accuracy Score: 0.25555555555555554\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Saving the SVM Classifier\n",
        "output_dir = MODELS_PATH\n",
        "model_filename = os.path.join(output_dir, svm_model_path)\n",
        "\n",
        "if not os.path.exists(output_dir):\n",
        "    os.makedirs(output_dir)\n",
        "\n",
        "svm_classifier.save_model(model_filename)"
      ],
      "metadata": {
        "id": "ekd4vWDrus7d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eddddd63-1ecc-45f2-916d-1bfd3a0a327f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-17 05:48:11,481 - INFO - Model saved to /content/drive/MyDrive/ML_Models/vit_b_16_svm_model.joblib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**End**"
      ],
      "metadata": {
        "id": "29tEaQcmvBJE"
      }
    }
  ]
}