{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNMymWQ4TwtC"
      },
      "source": [
        "# DL-CBIR using ViT Base-16 Architecture\n",
        "\n",
        "**Online Phase**\n",
        "***"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gODwD3yIU8LO"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tmyinl-SU5cr"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import logging\n",
        "from PIL import Image\n",
        "\n",
        "import ast\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import timm\n",
        "import torch\n",
        "from torch import nn\n",
        "import torch.nn.functional as F\n",
        "from torchvision import datasets, transforms, models\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "from torchvision.transforms import ToTensor\n",
        "\n",
        "import joblib\n",
        "from joblib import load\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "\n",
        "# Extras\n",
        "import time\n",
        "from google.colab import drive\n",
        "import matplotlib.pyplot as plt\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qWh9rVNhQiQw"
      },
      "outputs": [],
      "source": [
        "# HuggingFace Token\n",
        "os.environ['HF_TOKEN'] = userdata.get('HF_TOKEN')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rGJ1pdUB5Fbd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cab0a662-85b3-455b-dc2d-c9e48cf17809"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7a3556ed7c30>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "# Reproducibility\n",
        "np.random.seed(42)\n",
        "torch.manual_seed(42)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8-VYs4S8x9XJ"
      },
      "source": [
        "# Detect Device"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h6YkDAWvx8r1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d16edaf0-f008-4c37-e4a7-8532d5ff3347"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n"
          ]
        }
      ],
      "source": [
        "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(f\"Using device: {DEVICE}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "054K6K0oVB9_"
      },
      "source": [
        "# Configuration Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QBSGwcftU0DH"
      },
      "outputs": [],
      "source": [
        "DIR_PATH = '/content/drive/MyDrive/ML_Datasets/corel_1k_dataset/'\n",
        "FEATURES_PATH = '/content/drive/MyDrive/ML_Datasets/vit_b_16_feature_vectors.csv'\n",
        "MODELS_PATH = '/content/drive/MyDrive/ML_Models/'\n",
        "svm_model_path = MODELS_PATH + 'vit_b_16_svm_model.joblib'\n",
        "\n",
        "TRAIN_SPLIT = 0.8\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "N_WORKERS = 2\n",
        "\n",
        "PCA_DIM = 64\n",
        "N_RESULTS = 10"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5WPq4yBHVAOy"
      },
      "source": [
        "# Logging Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HdIlnNIWU2wY"
      },
      "outputs": [],
      "source": [
        "logger = logging.getLogger()\n",
        "logger.setLevel(logging.INFO)\n",
        "\n",
        "# Remove any existing handlers\n",
        "if logger.hasHandlers():\n",
        "    logger.handlers.clear()\n",
        "\n",
        "# Add a new stream handler\n",
        "stream_handler = logging.StreamHandler()\n",
        "stream_handler.setLevel(logging.INFO)\n",
        "formatter = logging.Formatter('%(asctime)s - %(levelname)s - %(message)s')\n",
        "stream_handler.setFormatter(formatter)\n",
        "logger.addHandler(stream_handler)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FcsMXUldo4We"
      },
      "source": [
        "# Connecting to the dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rhK9_jM9o3bu",
        "outputId": "87b68301-5d93-4664-cdda-4ebc985c5464"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "Available classes: ['flowers', 'bus', 'foods', 'monuments', 'dinosaurs', 'peolpe_and_villages_in_Africa', 'elephants', 'horses', 'beaches', 'mountains_and_snow']\n",
            "Number of classes: 10\n"
          ]
        }
      ],
      "source": [
        "drive.mount('/content/drive')\n",
        "print(f\"Available classes: {os.listdir(DIR_PATH + '/training_set')}\")\n",
        "print(f\"Number of classes: {len(os.listdir(DIR_PATH + '/training_set'))}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oUv4vOdA4Z2w"
      },
      "source": [
        "# Components"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KDzRRZip0jik"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_SBQrlzCu4Lg"
      },
      "outputs": [],
      "source": [
        "def get_data_transforms():\n",
        "    return transforms.Compose([\n",
        "        transforms.Resize((256, 256)),\n",
        "        transforms.CenterCrop(224),\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                             std=[0.229, 0.224, 0.225])\n",
        "    ])\n",
        "\n",
        "def load_images_and_labels(data_dir, target_class=None):\n",
        "    \"\"\"\n",
        "    Loads images (optionally only from a specific class) and returns:\n",
        "      - features (torch.Tensor) of shape (N, C, H, W) on DEVICE\n",
        "      - labels   (torch.Tensor) of shape (N,) on DEVICE\n",
        "      - file_names (list) of file paths for each image\n",
        "\n",
        "    Args:\n",
        "        data_dir (str): Directory path containing class subdirectories.\n",
        "        target_class (str or list[str], optional): Only load images from this class (or list of classes).\n",
        "                                                   If None, loads all classes.\n",
        "    \"\"\"\n",
        "    if not os.path.exists(data_dir):\n",
        "        raise FileNotFoundError(f\"Data directory {data_dir} not found.\")\n",
        "\n",
        "    transform_pipeline = get_data_transforms()\n",
        "    dataset = datasets.ImageFolder(root=data_dir)  # no transform yet\n",
        "\n",
        "    # Determine which class indices to keep\n",
        "    if target_class is None:\n",
        "        valid_class_idxs = set(dataset.class_to_idx.values())\n",
        "    else:\n",
        "        if isinstance(target_class, str):\n",
        "            target_list = [target_class]\n",
        "        else:\n",
        "            target_list = list(target_class)\n",
        "        valid_class_idxs = {dataset.class_to_idx[c] for c in target_list}\n",
        "\n",
        "    # Filter samples\n",
        "    filtered_samples = [\n",
        "        (path, label)\n",
        "        for (path, label) in dataset.samples\n",
        "        if label in valid_class_idxs\n",
        "    ]\n",
        "\n",
        "    feature_list = []\n",
        "    label_list = []\n",
        "    file_names  = []\n",
        "\n",
        "    for img_path, label in filtered_samples:\n",
        "        img = Image.open(img_path).convert('RGB')\n",
        "        img_t = transform_pipeline(img)\n",
        "        feature_list.append(img_t)\n",
        "        label_list.append(label)\n",
        "        file_names.append(img_path)\n",
        "\n",
        "    # Stack and move to DEVICE\n",
        "    features = torch.stack(feature_list).to(DEVICE)      # (N, C, H, W)\n",
        "    labels   = torch.tensor(label_list, device=DEVICE)   # (N,)\n",
        "\n",
        "    return features, labels, file_names"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V0nBA_U0rmU5"
      },
      "source": [
        "## Pre-trained ViT Base-16 Model for Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iYD8aHJzri8S"
      },
      "outputs": [],
      "source": [
        "class ViTFeatureExtractor(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ViTFeatureExtractor, self).__init__()\n",
        "        # Load pre‑trained ViT and remove its classification head\n",
        "        self.model = timm.create_model('vit_base_patch16_224', pretrained=True)\n",
        "        self.model.head = nn.Identity()\n",
        "        # Move to DEVICE and set to eval mode\n",
        "        self.model.to(DEVICE)\n",
        "        self.model.eval()\n",
        "\n",
        "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Extracts ViT features for a batch of images.\n",
        "\n",
        "        Args:\n",
        "            x (torch.Tensor): Batch of images as a tensor of shape (N, C, H, W),\n",
        "                              already resized/cropped/normalized to 224×224.\n",
        "\n",
        "        Returns:\n",
        "            torch.Tensor: Feature tensor of shape (N, embed_dim) on DEVICE.\n",
        "        \"\"\"\n",
        "        # Ensure input is on DEVICE\n",
        "        x = x.to(DEVICE)\n",
        "        with torch.no_grad():\n",
        "            features = self.model(x)\n",
        "        return features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_qqgynMdUoQt"
      },
      "source": [
        "## PCA for Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hMzK-o7TUrBv"
      },
      "outputs": [],
      "source": [
        "def compute_normalized_pca(features: torch.Tensor, reduced_dim: int) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Performs PCA-based dimensionality reduction on a tensor of AlexNet features (N, 4096)\n",
        "    followed by L2 normalization. The output always has shape (N, reduced_di'flowers'm), even if the\n",
        "    number of available principal components (i.e. batch size N) is less than reduced_dim.\n",
        "    In such cases, the projection matrix is padded with zeros. All computation happens on DEVICE.\n",
        "\n",
        "    Args:\n",
        "        features (torch.Tensor): Input tensor of shape (N, 4096), where N is the number of images.\n",
        "        reduced_dim (int): Target dimensionality after PCA.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: L2‑normalized reduced features of shape (N, reduced_dim), on DEVICE.\n",
        "    \"\"\"\n",
        "    # Move features to DEVICE\n",
        "    features = features.to(DEVICE)\n",
        "\n",
        "    # Step 1: Center the data\n",
        "    mean = features.mean(dim=0, keepdim=True)               # (1, 4096)\n",
        "    features_centered = features - mean                     # (N, 4096)\n",
        "\n",
        "    # Step 2: Compute SVD on the centered data\n",
        "    U, S, Vh = torch.linalg.svd(features_centered, full_matrices=False)\n",
        "    # Vh has shape (min(N, 4096), 4096)\n",
        "    num_components = Vh.shape[0]\n",
        "\n",
        "    # Step 3: Select top principal components and form projection matrix\n",
        "    if num_components >= reduced_dim:\n",
        "        # Enough components\n",
        "        principal_components = Vh[:reduced_dim].T          # (4096, reduced_dim)\n",
        "    else:\n",
        "        # Pad with zeros\n",
        "        principal_components = Vh[:num_components].T       # (4096, num_components)\n",
        "        pad_width = reduced_dim - num_components\n",
        "        padding = torch.zeros((features.shape[1], pad_width), device=DEVICE)\n",
        "        principal_components = torch.cat([principal_components, padding], dim=1)  # (4096, reduced_dim)\n",
        "\n",
        "    # Step 4: Project onto the reduced subspace\n",
        "    reduced_features = features_centered @ principal_components  # (N, reduced_dim)\n",
        "\n",
        "    # Step 5: L2‑normalize each row\n",
        "    reduced_features_normalized = F.normalize(reduced_features, p=2, dim=1)  # (N, reduced_dim)\n",
        "\n",
        "    return reduced_features_normalized\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zh98juc2VH1l"
      },
      "source": [
        "## DCT for Feature Extraction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_O_HlvycVY6D"
      },
      "outputs": [],
      "source": [
        "def compute_normalized_dct(images: torch.Tensor) -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes the L2‑normalized DCT feature vector for each image in a batch,\n",
        "    returning a tensor on DEVICE.\n",
        "\n",
        "    Args:\n",
        "        images (torch.Tensor): Batch of input images with shape (N, C, H, W),\n",
        "                               where N is the number of images and C is the number of channels (3).\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Tensor of L2‑normalized DCT feature vectors with shape (N, feature_length),\n",
        "                      on DEVICE.\n",
        "    \"\"\"\n",
        "    # Move to CPU and convert to HWC numpy array\n",
        "    images_np = images.permute(0, 2, 3, 1).cpu().numpy()  # shape: (N, H, W, C)\n",
        "\n",
        "    dct_features = []\n",
        "    for img in images_np:\n",
        "        # Grayscale\n",
        "        gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
        "        # float32\n",
        "        gray_f = np.float32(gray)\n",
        "        # DCT\n",
        "        dct_res = cv2.dct(gray_f)\n",
        "        # Flatten\n",
        "        vec = dct_res.flatten()\n",
        "        # L2 norm\n",
        "        norm = np.linalg.norm(vec)\n",
        "        if norm > 0:\n",
        "            vec = vec / norm\n",
        "        dct_features.append(vec)\n",
        "\n",
        "    # Stack into numpy, convert to torch tensor, move to DEVICE\n",
        "    dct_np = np.stack(dct_features, axis=0)           # shape: (N, feature_length)\n",
        "    dct_tensor = torch.from_numpy(dct_np).to(DEVICE)  # on DEVICE\n",
        "\n",
        "    return dct_tensor"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RsmwicLVsv0q"
      },
      "source": [
        "## Feature Vector Combination"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4TVigE3Zs01h"
      },
      "outputs": [],
      "source": [
        "def compute_combined_feature_vector(\n",
        "    images_tensor: torch.Tensor,\n",
        "    alexnet_features: torch.Tensor,\n",
        "    pca_dim: int = PCA_DIM\n",
        ") -> torch.Tensor:\n",
        "    \"\"\"\n",
        "    Computes Combined Feature Vectors by concatenating the normalized DCT and PCA feature vectors\n",
        "    for a batch of images, performing all computations on DEVICE.\n",
        "\n",
        "    Args:\n",
        "        images_tensor (torch.Tensor): Input image tensor with shape (N, C, H, W).\n",
        "        alexnet_features (torch.Tensor): AlexNet feature tensor with shape (N, 4096).\n",
        "        pca_dim (int): Target dimensionality for PCA reduction.\n",
        "\n",
        "    Returns:\n",
        "        torch.Tensor: Combined feature vectors of shape (N, dct_length + pca_dim), on DEVICE.\n",
        "    \"\"\"\n",
        "    # Move inputs to DEVICE\n",
        "    images_tensor = images_tensor.to(DEVICE)\n",
        "    alexnet_features = alexnet_features.to(DEVICE)\n",
        "\n",
        "    # Compute normalized DCT feature vectors (on DEVICE)\n",
        "    dct_features = compute_normalized_dct(images_tensor)    # shape: (N, dct_length)\n",
        "\n",
        "    # Compute normalized PCA feature vectors (on DEVICE)\n",
        "    pca_features = compute_normalized_pca(alexnet_features, pca_dim)  # shape: (N, pca_dim)\n",
        "\n",
        "    # Concatenate along feature axis (result is on DEVICE)\n",
        "    combined_feature_vectors = torch.cat((dct_features, pca_features), dim=1)\n",
        "\n",
        "    return combined_feature_vectors"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jLrti68nCcmM"
      },
      "source": [
        "## Similarity Check"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AVeqUtvRWIB6"
      },
      "source": [
        "### SVM Model for prediction of Image Class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1-SvHt0rWKSs"
      },
      "outputs": [],
      "source": [
        "class SVMClassifier:\n",
        "    def __init__(self, csv_file: str, label_column: str = 'label', test_size: float = 0.2, random_state: int = 42):\n",
        "        \"\"\"\n",
        "        Initializes the SVM classifier with data from a CSV file.\n",
        "\n",
        "        Args:\n",
        "            csv_file (str): Path to the input CSV file.\n",
        "            label_column (str): Name of the column containing class labels.\n",
        "            test_size (float): Fraction of data to be used for testing (default: 0.2).\n",
        "            random_state (int): Random seed for reproducibility (default: 42).\n",
        "        \"\"\"\n",
        "        self.csv_file = csv_file\n",
        "        self.label_column = label_column\n",
        "        self.test_size = test_size\n",
        "        self.random_state = random_state\n",
        "        self.model = None\n",
        "        self.scaler = None\n",
        "        self.X_test = None\n",
        "        self.y_test = None\n",
        "        logging.info(f\"SVMClassifier initialized with csv_file: {csv_file}, label_column: {label_column}\")\n",
        "\n",
        "    def load_data(self):\n",
        "        \"\"\"\n",
        "        Loads data from the CSV file and splits it into training and testing sets.\n",
        "        Expects the CSV file to have columns: file_path, label, f1, f2, ..., fN.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (X_train, X_test, y_train, y_test)\n",
        "        \"\"\"\n",
        "        logging.info(\"Loading data...\")\n",
        "        # Load the dataset\n",
        "        data = pd.read_csv(self.csv_file)\n",
        "\n",
        "        # Drop the file_path column if it exists (we don't use it for training)\n",
        "        if 'file_path' in data.columns:\n",
        "            data.drop(columns=['file_path'], inplace=True)\n",
        "\n",
        "        # Features are assumed to be all columns except the label column.\n",
        "        features = data.drop(columns=[self.label_column])\n",
        "        labels = data[self.label_column]\n",
        "\n",
        "        # Split into training and testing sets.\n",
        "        X_train, X_test, y_train, y_test = train_test_split(\n",
        "            features, labels, test_size=self.test_size, random_state=self.random_state\n",
        "        )\n",
        "        logging.info(\"Data loaded and split into training and test sets.\")\n",
        "        return X_train, X_test, y_train, y_test\n",
        "\n",
        "    def train(self):\n",
        "        \"\"\"\n",
        "        Trains the SVM model on the training set.\n",
        "        Standardizes the features and stores the trained model and scaler as instance attributes.\n",
        "\n",
        "        Returns:\n",
        "            model: The trained SVM model.\n",
        "        \"\"\"\n",
        "        logging.info(\"Starting training...\")\n",
        "        # Load and split the data.\n",
        "        X_train, X_test, y_train, y_test = self.load_data()\n",
        "\n",
        "        # Standardize features for better SVM performance.\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_test_scaled = scaler.transform(X_test)\n",
        "        self.scaler = scaler\n",
        "        self.X_test = X_test_scaled\n",
        "        self.y_test = y_test\n",
        "\n",
        "        # Initialize the SVM classifier with RBF kernel.\n",
        "        model = SVC(kernel='rbf', gamma='scale', C=1.0)\n",
        "        logging.info(\"SVM model initialized with RBF kernel.\")\n",
        "\n",
        "        # Train the model.\n",
        "        model.fit(X_train_scaled, y_train)\n",
        "        self.model = model\n",
        "        logging.info(\"SVM model training completed.\")\n",
        "        return model\n",
        "\n",
        "    def evaluate(self):\n",
        "        \"\"\"\n",
        "        Evaluates the trained SVM model on the test set.\n",
        "        Prints the classification report and accuracy score.\n",
        "\n",
        "        Returns:\n",
        "            tuple: (report, accuracy)\n",
        "        \"\"\"\n",
        "        if self.model is None:\n",
        "            logging.error(\"Model not trained. Please train the model before evaluation.\")\n",
        "            return None, None\n",
        "\n",
        "        logging.info(\"Starting evaluation...\")\n",
        "        # Make predictions on the test set.\n",
        "        y_pred = self.model.predict(self.X_test)\n",
        "\n",
        "        # Evaluate the model.\n",
        "        report = classification_report(self.y_test, y_pred)\n",
        "        accuracy = accuracy_score(self.y_test, y_pred)\n",
        "        logging.info(\"Evaluation completed.\")\n",
        "\n",
        "        print(\"Classification Report:\")\n",
        "        print(report)\n",
        "        print(\"Accuracy Score:\", accuracy)\n",
        "        return report, accuracy\n",
        "\n",
        "    def save_model(self, filename: str):\n",
        "        \"\"\"\n",
        "        Saves the trained model and scaler to a file using joblib.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The file path where the model will be saved.\n",
        "        \"\"\"\n",
        "        if self.model is None or self.scaler is None:\n",
        "            logging.error(\"No trained model to save. Please train the model first.\")\n",
        "            return\n",
        "\n",
        "        joblib.dump((self.model, self.scaler), filename)\n",
        "        logging.info(f\"Model saved to {filename}\")\n",
        "\n",
        "    def load_model(self, filename: str):\n",
        "        \"\"\"\n",
        "        Loads a model and scaler from a file using joblib.\n",
        "\n",
        "        Args:\n",
        "            filename (str): The file path from where the model will be loaded.\n",
        "        \"\"\"\n",
        "        self.model, self.scaler = joblib.load(filename)\n",
        "        logging.info(f\"Model loaded from {filename}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vQjjX0EXanKJ"
      },
      "source": [
        "### SimilarityCheck class"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yMJ7HGRaCgrU"
      },
      "outputs": [],
      "source": [
        "class SimilarityCheck:\n",
        "    def __init__(self, csv_file: str, svm_model_file: str, label_column: str = 'label'):\n",
        "        \"\"\"\n",
        "        Initializes the SimilarityCheck object.\n",
        "\n",
        "        Args:\n",
        "            csv_file (str): Path to the CSV file containing image features, labels, and file paths.\n",
        "            svm_model_file (str): Path to the saved SVM model file.\n",
        "            label_column (str): Name of the column containing class labels.\n",
        "        \"\"\"\n",
        "        self.csv_file = csv_file\n",
        "        self.label_column = label_column\n",
        "        self.svm_classifier = None\n",
        "        self.load_svm(svm_model_file)\n",
        "        # Load database once, on DEVICE\n",
        "        self.db_vectors, self.db_labels, self.db_paths = self.load_feature_database()\n",
        "        logging.info(\"SimilarityCheck initialized.\")\n",
        "\n",
        "    def load_svm(self, model_file: str):\n",
        "        \"\"\"\n",
        "        Loads the SVM classifier from a saved model file.\n",
        "        (scikit-learn remains on CPU)\n",
        "        \"\"\"\n",
        "        self.svm_classifier, _ = load(model_file)\n",
        "        logging.info(f\"SVM model loaded from {model_file}.\")\n",
        "\n",
        "    def load_feature_database(self):\n",
        "        \"\"\"\n",
        "        Loads the image database from a CSV file onto DEVICE.\n",
        "        Expects columns: file_path, label, f1, f2, ..., fN.\n",
        "        Returns:\n",
        "            db_vectors (torch.Tensor): (M, d) on DEVICE\n",
        "            db_labels  (np.ndarray): length M\n",
        "            db_paths   (list[str]) : length M\n",
        "        \"\"\"\n",
        "        df = pd.read_csv(self.csv_file)\n",
        "        paths = df['file_path'].tolist() if 'file_path' in df.columns else [''] * len(df)\n",
        "        labels = df[self.label_column].to_numpy()\n",
        "        feats = df.drop(columns=['file_path', self.label_column]).to_numpy(dtype=np.float32)\n",
        "        # move to torch tensor on DEVICE\n",
        "        feats_t = torch.from_numpy(feats).to(DEVICE)\n",
        "        logging.info(\"Feature database loaded onto DEVICE.\")\n",
        "        return feats_t, labels, paths\n",
        "\n",
        "    def compute_euclidean_distances(self, query_vector: torch.Tensor) -> torch.Tensor:\n",
        "        \"\"\"\n",
        "        Computes Euclidean distances on DEVICE between query_vector and each db vector.\n",
        "        Args:\n",
        "            query_vector (torch.Tensor): shape (d,) or (1, d), on CPU or DEVICE.\n",
        "        Returns:\n",
        "            distances (torch.Tensor): shape (M,), on DEVICE.\n",
        "        \"\"\"\n",
        "        # ensure 1D tensor on DEVICE\n",
        "        q = query_vector.detach().to(DEVICE)\n",
        "        if q.ndim == 1:\n",
        "            q = q.unsqueeze(0)  # (1, d)\n",
        "        # broadcast-subtract\n",
        "        diff = self.db_vectors - q        # (M, d)\n",
        "        dists = torch.norm(diff, dim=1)   # (M,)\n",
        "        return dists\n",
        "\n",
        "    def get_top_n_similar(self, query_vector, top_n: int = 5):\n",
        "        \"\"\"\n",
        "        Returns top_n indices and distances as numpy arrays.\n",
        "        \"\"\"\n",
        "        # convert numpy->torch if needed\n",
        "        if isinstance(query_vector, np.ndarray):\n",
        "            qv = torch.from_numpy(query_vector).to(DEVICE)\n",
        "        else:\n",
        "            qv = query_vector.to(DEVICE)\n",
        "        dists = self.compute_euclidean_distances(qv)          # (M,)\n",
        "        vals, idxs = torch.topk(dists, k=top_n, largest=False)\n",
        "        return idxs.cpu().numpy(), vals.cpu().numpy()\n",
        "\n",
        "    def perform_similarity_search(self, query_vector, top_n: int = 5):\n",
        "        \"\"\"\n",
        "        Returns list of dicts with file_path, label, distance.\n",
        "        \"\"\"\n",
        "        idxs, dists = self.get_top_n_similar(query_vector, top_n)\n",
        "        results = []\n",
        "        for idx, dist in zip(idxs, dists):\n",
        "            results.append({\n",
        "                'file_path': self.db_paths[idx],\n",
        "                'label':    self.db_labels[idx],\n",
        "                'distance': float(dist)\n",
        "            })\n",
        "        logging.info(\"Similarity search completed.\")\n",
        "        return results\n",
        "\n",
        "    def predict_class(self, query_vector) -> str:\n",
        "        \"\"\"\n",
        "        Predicts class via CPU-based sklearn SVM.\n",
        "        \"\"\"\n",
        "        # convert to 1D numpy\n",
        "        if isinstance(query_vector, torch.Tensor):\n",
        "            qn = query_vector.detach().cpu().numpy()\n",
        "        else:\n",
        "            qn = np.array(query_vector)\n",
        "        pred = self.svm_classifier.predict([qn])[0]\n",
        "        logging.info(f\"Predicted class: {pred}\")\n",
        "        return pred\n",
        "\n",
        "    def check_similarity(self, query_vector, top_n: int = 5):\n",
        "        \"\"\"\n",
        "        Combined prediction and similarity.\n",
        "        \"\"\"\n",
        "        cls = self.predict_class(query_vector)\n",
        "        sims = self.perform_similarity_search(query_vector, top_n)\n",
        "        return cls, sims"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srxhOFhe0ObU"
      },
      "source": [
        "## Evaluation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tAoRvsa0hNC"
      },
      "outputs": [],
      "source": [
        "def compute_map(\n",
        "    similarity_checker,\n",
        "    query_vectors,\n",
        "    query_labels,\n",
        "    top_k: int = 5\n",
        ") -> float:\n",
        "    \"\"\"\n",
        "    Computes mean Average Precision (mAP) for a set of query vectors against\n",
        "    the database loaded inside similarity_checker.\n",
        "\n",
        "    Args:\n",
        "        similarity_checker: Your SimilarityCheck instance.\n",
        "        query_vectors (np.ndarray or torch.Tensor): (N, d) feature vectors.\n",
        "        query_labels  (list, np.ndarray, or torch.Tensor): length-N labels.\n",
        "        top_k         (int): Number of neighbors to consider per query.\n",
        "\n",
        "    Returns:\n",
        "        float: mean Average Precision over all queries.\n",
        "    \"\"\"\n",
        "    # 1) Bring query_vectors to CPU numpy\n",
        "    if isinstance(query_vectors, torch.Tensor):\n",
        "        query_vectors = query_vectors.detach().cpu().numpy()\n",
        "\n",
        "    # 2) Bring query_labels to a 1D numpy array\n",
        "    if isinstance(query_labels, torch.Tensor):\n",
        "        query_labels = query_labels.detach().cpu().numpy()\n",
        "    else:\n",
        "        query_labels = np.array(query_labels)\n",
        "\n",
        "    N = query_vectors.shape[0]\n",
        "    if N == 0:\n",
        "        return 0.0\n",
        "\n",
        "    APs = []\n",
        "    for i in range(N):\n",
        "        qv = query_vectors[i]        # (d,)\n",
        "        ql = query_labels[i]\n",
        "\n",
        "        # retrieve top_k + 1 so we can drop a self‐match\n",
        "        idxs, _ = similarity_checker.get_top_n_similar(qv, top_k + 1)\n",
        "\n",
        "        # drop the query itself if present\n",
        "        idxs = [j for j in idxs if j != i]\n",
        "\n",
        "        # limit to top_k\n",
        "        idxs = idxs[:top_k]\n",
        "\n",
        "        # get their labels\n",
        "        retrieved = [similarity_checker.db_labels[j] for j in idxs]\n",
        "\n",
        "        # compute AP\n",
        "        num_hits = 0\n",
        "        precisions = []\n",
        "        for rank, lbl in enumerate(retrieved, start=1):\n",
        "            if lbl == ql:\n",
        "                num_hits += 1\n",
        "                precisions.append(num_hits / rank)\n",
        "\n",
        "        APs.append(np.mean(precisions) if precisions else 0.0)\n",
        "\n",
        "    return float(np.mean(APs))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nBITuZFTRuAQ"
      },
      "source": [
        "## Display Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OpSFBU9qMbu5"
      },
      "outputs": [],
      "source": [
        "def display_similar_images(similar_images):\n",
        "    \"\"\"\n",
        "    Displays images along with their label and distance values in two columns.\n",
        "\n",
        "    Args:\n",
        "        similar_images (list of dict): A list of dictionaries where each dictionary contains:\n",
        "            - 'file_path': Path to the image file\n",
        "            - 'label': Label of the image\n",
        "            - 'distance': Distance value of the image\n",
        "    \"\"\"\n",
        "    # Number of images\n",
        "    num_images = len(similar_images)\n",
        "\n",
        "    # Create a figure with two columns\n",
        "    fig, axes = plt.subplots(nrows=(num_images + 1) // 2, ncols=2, figsize=(10, 5 * ((num_images + 1) // 2)))\n",
        "    axes = axes.flatten()  # Flatten axes for easy iteration\n",
        "\n",
        "    for idx, image_info in enumerate(similar_images):\n",
        "        file_path = image_info['file_path']\n",
        "\n",
        "        if not os.path.exists(file_path):\n",
        "            print(f\"File not found: {file_path}. Skipping...\")\n",
        "            axes[idx].axis('off')\n",
        "            continue\n",
        "\n",
        "        # Load the image\n",
        "        img = Image.open(file_path)\n",
        "\n",
        "        # Display the image\n",
        "        axes[idx].imshow(img)\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "        # Add label and distance as title\n",
        "        axes[idx].set_title(f\"Label: {image_info['label']}\\nDistance: {image_info['distance']:.4f}\", fontsize=10)\n",
        "\n",
        "    # Hide any unused subplots\n",
        "    for idx in range(len(similar_images), len(axes)):\n",
        "        axes[idx].axis('off')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tRjn5grhuK7Z"
      },
      "source": [
        "# Result"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ku-ZmxE6ZgKl",
        "outputId": "a7ff504d-7392-4a86-b933-7d87b5052428"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-24 03:44:23,057 - INFO - Time taken to execute = 0.3149s\n"
          ]
        }
      ],
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Load ALL classes:\n",
        "# query_images, query_labels, query_file_paths = load_images_and_labels(DIR_PATH + '/test_set')\n",
        "\n",
        "# # Load only the BUS class:\n",
        "# features_bus, labels_bus, paths_bus = load_images_and_labels(\n",
        "#     DIR_PATH + '/test_set', target_class='bus'\n",
        "# )\n",
        "\n",
        "# # Load classes BUS and FLOWERS:\n",
        "query_images, query_labels, query_file_paths = load_images_and_labels(\n",
        "    DIR_PATH + '/test_set', target_class=['bus','flowers']\n",
        ")\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6qIx2LiiZm2h"
      },
      "outputs": [],
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# ViT Model for Feature Extraction\n",
        "model = ViTFeatureExtractor()\n",
        "# The model expects a tensor of shape (N, C, H, W) and returns features of shape (N, 4096).\n",
        "model_features = model(query_images)\n",
        "logging.info(f\"Extracted features shape: {model_features.shape}\")\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Xa4YkhIWZtK8",
        "outputId": "4e93ae86-fef1-410a-bc86-b760e8a48754"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-24 03:44:28,502 - INFO - Time taken to execute = 0.3226s\n"
          ]
        }
      ],
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Combined Feature Vector Generation\n",
        "query_vectors = compute_combined_feature_vector(query_images, model_features, PCA_DIM)\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rOxmR__Rutsq",
        "outputId": "9ae8bd93-2f46-4a8f-b272-1c67840ec494"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "2025-04-24 03:44:37,154 - INFO - SVM model loaded from /content/drive/MyDrive/ML_Models/vit_b_16_svm_model.joblib.\n",
            "2025-04-24 03:45:22,468 - INFO - Feature database loaded onto DEVICE.\n",
            "2025-04-24 03:45:22,478 - INFO - SimilarityCheck initialized.\n",
            "2025-04-24 03:45:22,928 - INFO - Predicted class: 4\n",
            "2025-04-24 03:45:23,039 - INFO - Similarity search completed.\n",
            "2025-04-24 03:45:23,043 - INFO - Time taken to execute = 36.5467s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "For image: /content/drive/MyDrive/ML_Datasets/corel_1k_dataset//test_set/bus/300.jpg\n",
            "Predicted Class: 4\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Start time of execution\n",
        "start_time = time.process_time()\n",
        "\n",
        "# Perform the Similarity Check using the pre-trained SVM Classifier\n",
        "results = [] # Results of all query images\n",
        "similarity_checker = SimilarityCheck(csv_file=FEATURES_PATH, svm_model_file=svm_model_path)\n",
        "similarity_checker.model, similarity_checker.scaler = joblib.load(svm_model_path)\n",
        "\n",
        "# # Load Features Database\n",
        "# db_vectors, db_labels, db_file_paths = similarity_checker.load_feature_database()\n",
        "\n",
        "# Perform similarity search for each image and print the top similar images.\n",
        "for i, query_vector in enumerate(query_vectors):\n",
        "    predicted_class, similar_images = similarity_checker.check_similarity(query_vector, N_RESULTS)\n",
        "    print(f\"\\nFor image: {query_file_paths[i]}\")\n",
        "    print(\"Predicted Class:\", predicted_class, end='\\n\\n')\n",
        "    results.append(similar_images)\n",
        "    break # Uncomment if displaying all query results\n",
        "\n",
        "# Total Time for execution\n",
        "logging.info(f\"Time taken to execute = {(time.process_time() - start_time):.4f}s\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vRUN-1cAXuEJ",
        "outputId": "307d073e-bfd7-476b-8c05-527fdeabf147"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mean Average Precision (mAP): 0.1716\n"
          ]
        }
      ],
      "source": [
        "# Compute Mean Average Precision\n",
        "map_score = compute_map(similarity_checker, query_vectors, query_labels, top_k=N_RESULTS)\n",
        "print(f\"Mean Average Precision (mAP): {map_score:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-pTbGqOHMdAQ"
      },
      "source": [
        "# Displaying the results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ELqzGRV-QWD9"
      },
      "outputs": [],
      "source": [
        "input_image = Image.open(query_file_paths[0])\n",
        "plt.imshow(input_image)\n",
        "plt.axis('off')\n",
        "plt.title('Input Image')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qhGkeEDxR3Hr"
      },
      "outputs": [],
      "source": [
        "display_similar_images(similar_images)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "29tEaQcmvBJE"
      },
      "source": [
        "**End**"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "oUv4vOdA4Z2w",
        "nBITuZFTRuAQ"
      ],
      "provenance": [],
      "toc_visible": true,
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}